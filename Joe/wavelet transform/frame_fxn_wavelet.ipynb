{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavelet Transform and Run-Length Encoding\n",
    "## Compression\n",
    "#### compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_compress(sampling_rate, num_bits_run_len, max_prd, thresh_perc_approx,\n",
    "                        thresh_perc_d5, thresh_perc_d4_d1, data):\n",
    "    import pywt\n",
    "    from pywt import wavedec\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from scipy.signal import detrend\n",
    "    import copy \n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    FS = sampling_rate\n",
    "    COEFF_LENGTHS = {'cA5': 100, 'cD5': 100, 'cD4': 100, 'cD3': 100, 'cD2': 100, 'cD1': 100}\n",
    "    NUM_BITS_RUN_LEN = num_bits_run_len\n",
    "    MAX_PRD = max_prd\n",
    "    THRESH_PERC_APPROX = thresh_perc_approx\n",
    "    THRESH_PERC_D5 = thresh_perc_d5\n",
    "    THRESH_PERC_D4_D1 = thresh_perc_d4_d1\n",
    "    NUM_SAMPLES = len(data)\n",
    "    \n",
    "    coeffs = wavelet_decomposition(data)\n",
    "    for key in coeffs.keys():\n",
    "        COEFF_LENGTHS[key] = len(coeffs[key])\n",
    "    \n",
    "    coeffs_thresholded, binary_map, coeffs_orig = threshold_energy(coeffs)\n",
    "    \n",
    "    coeffs_scaled, scaling_factors = scale_coeffs(coeffs_thresholded)\n",
    "    \n",
    "    num_bits, PRD = calculate_num_bits(data, coeffs_scaled, binary_map, scaling_factors)\n",
    "    coeffs_quantized = do_quantization(coeffs_scaled, num_bits)\n",
    "    \n",
    "    if num_bits < 9:\n",
    "        coeffs_quantized_combined = combine_coefficients(coeffs_quantized, binary_map)\n",
    "    binary_map_combined = combine_coefficients(binary_map)\n",
    "    \n",
    "    coeffs_quantized_compressed, num_bits_last_byte_coeffs = compress_coefficients(coeffs_quantized_combined, num_bits)\n",
    "    binary_map_initial_state, binary_map_compressed, num_bits_last_byte_binary_map = compress_binary_map(binary_map_combined)\n",
    "    \n",
    "    os.mkdir('compressed_ppg')\n",
    "    if num_bits < 9:\n",
    "        np.savetxt('compressed_ppg/ppg_cqc.txt',coeffs_quantized_compressed,fmt='%3d') \n",
    "        with open('compressed_ppg/ppg_nblbc.txt','w') as f:\n",
    "            f.write('%d' % num_bits_last_byte_coeffs)\n",
    "    else:\n",
    "        np.savetxt('compressed_ppg/ppg_cqc.txt',coeffs_quantized_combined,fmt='%3d') \n",
    "\n",
    "    with open('compressed_ppg/ppg_bmis.txt','w') as f:\n",
    "        f.write('%d' % binary_map_initial_state)\n",
    "    np.savetxt('compressed_ppg/ppg_bmc.txt',binary_map_compressed,fmt='%3d') \n",
    "    with open('compressed_ppg/ppg_nblbbm.txt','w') as f:\n",
    "        f.write('%d' % num_bits_last_byte_binary_map)\n",
    "    with open('compressed_ppg/ppg_nb.txt','w') as f:\n",
    "        f.write('%d' % num_bits)\n",
    "    json.dump(scaling_factors, open(\"compressed_ppg/ppg_sf.txt\",'w'))\n",
    "    json.dump(COEFF_LENGTHS, open(\"compressed_ppg/ppg_cl.txt\",'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompression\n",
    "\n",
    "#### read compressed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_quantized_compressed = np.loadtxt('compressed_eda/eda_cqc.txt').astype('int32')\n",
    "\n",
    "with open('compressed_eda/eda_nblbc.txt','r') as f:\n",
    "    num_bits_last_byte_coeffs = int(f.read())\n",
    "\n",
    "with open('compressed_eda/eda_bmis.txt','r') as f:\n",
    "    binary_map_initial_state = int(f.read())\n",
    "\n",
    "binary_map_compressed = np.loadtxt('compressed_eda/eda_bmc.txt').astype('int32')\n",
    "\n",
    "with open('compressed_eda/eda_nblbbm.txt','r') as f:\n",
    "    num_bits_last_byte_binary_map = int(f.read())\n",
    "\n",
    "with open('compressed_eda/eda_nb.txt','r') as f:\n",
    "    num_bits = int(f.read())\n",
    "\n",
    "scaling_factors = json.load(open('compressed_eda/eda_sf.txt'))\n",
    "\n",
    "COEFF_LENGTHS = json.load(open(\"compressed_eda/eda_cl.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decompress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_decompress(coeffs_quantized_compressed, num_bits_last_byte_coeffs, binary_map_initial_state,\n",
    "                       binary_map_compressed, num_bits_last_byte_binary_map, num_bits,\n",
    "                       scaling_factors, COEFF_LENGTHS):\n",
    "    \n",
    "    binary_map_decompressed = decompress_binary_map(binary_map_compressed, binary_map_initial_state, num_bits_last_byte_binary_map)\n",
    "    coeffs_decompressed = decompress_coefficients(coeffs_quantized_compressed, num_bits, num_bits_last_byte_coeffs)\n",
    "    coeffs_reconstructed = remap_coeffs(coeffs_decompressed, binary_map_decompressed)\n",
    "    coeffs_unscaled = unscale_coeffs(coeffs_reconstructed, scaling_factors, num_bits)\n",
    "    data_reconstructed = wavelet_reconstruction(coeffs_unscaled)\n",
    "    \n",
    "    return data_reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "## Sub-functions Defined \n",
    "\n",
    "### wavelet decomposition\n",
    "*The wavelet used in this algorithm is bior4.4, with 5 levels of decomposition.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_decomposition(sig):\n",
    "    cA5, cD5, cD4, cD3, cD2, cD1 = wavedec(sig, 'bior4.4', level=5)\n",
    "    coeffs = {'cA5': cA5, 'cD5': cD5, 'cD4': cD4, 'cD3': cD3, 'cD2': cD2, 'cD1': cD1}\n",
    "\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wavelet reconstruction\n",
    "*the wavelet is bior4.4 with 5 levels of decomposition*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_reconstruction(coeffs):\n",
    "    reconstructed = pywt.waverec([coeffs['cA5'], coeffs['cD5'], coeffs['cD4'], coeffs['cD3'], \n",
    "                                    coeffs['cD2'], coeffs['cD1']], 'bior4.4')\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threshold wavelet coefficients to a percentage of total energy\n",
    "*Different levels of decomposition are thresholded at different energy percentages.*  \n",
    "1. calculate the energy of all the coefficients\n",
    "1. compute threshold for each coefficient matrix\n",
    "1. keep corresponding coefficients that are above the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_energy(coeffs):\n",
    "    #make a deep copy of coeffs to retain the original version\n",
    "    coeffs_orig = copy.deepcopy(coeffs)\n",
    "\n",
    "    binary_map = {}\n",
    "    nonzero_coeff_count = {}\n",
    "\n",
    "    for key in coeffs.keys():\n",
    "        #sort the absolute value of the coefficients in descending order\n",
    "        tmp_coeffs = np.sort(np.abs(coeffs[key]))[::-1]\n",
    "\n",
    "        #calculate the threshold for retaining some percentage of the energy\n",
    "        if key == 'cA5':\n",
    "            thresh_perc = THRESH_PERC_APPROX\n",
    "        elif key == 'cD5':\n",
    "            thresh_perc = THRESH_PERC_D5\n",
    "        else:\n",
    "            thresh_perc = THRESH_PERC_D4_D1\n",
    "\n",
    "        energy_thresholded = thresh_perc*energy(tmp_coeffs)\n",
    "        energy_tmp = 0\n",
    "        for coeff in tmp_coeffs:\n",
    "            energy_tmp = energy_tmp + coeff**2\n",
    "\n",
    "            if energy_tmp >= energy_thresholded:\n",
    "                threshold = coeff\n",
    "                break\n",
    "\n",
    "        #set any coefficients below the threshold to zero\n",
    "        tmp_coeffs = coeffs[key]\n",
    "        inds_to_zero = np.where((tmp_coeffs < threshold) & (tmp_coeffs > -threshold))[0]\n",
    "        tmp_coeffs[inds_to_zero] = 0\n",
    "\n",
    "        #create the binary map\n",
    "        binary_map_tmp = np.ones(len(coeffs[key])).astype(int)\n",
    "        binary_map_tmp[inds_to_zero] = 0\n",
    "\n",
    "        #update the various dictionaries\n",
    "        coeffs[key] = tmp_coeffs\n",
    "        binary_map[key] = binary_map_tmp\n",
    "        nonzero_coeff_count[key] = len(tmp_coeffs)\n",
    "        \n",
    "    return coeffs, binary_map, coeffs_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scale the wavelet coefficients to the [0,1] range\n",
    "*two scaling factors: a shift factor and a multiplication factor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_coeffs(coeffs):\n",
    "    coeffs_scaled = {}\n",
    "    scaling_factors = {}\n",
    "\n",
    "    for key in coeffs.keys():\n",
    "        shift_factor = np.min(coeffs[key])\n",
    "        coeffs_tmp = coeffs[key]-shift_factor\n",
    "\n",
    "        scale_factor = np.max(coeffs_tmp)\n",
    "        coeffs_tmp = coeffs_tmp/scale_factor\n",
    "\n",
    "        scaling_factors[key] = {'shift_factor': shift_factor, 'scale_factor': scale_factor}\n",
    "        coeffs_scaled[key] = coeffs_tmp\n",
    "\n",
    "    return coeffs_scaled, scaling_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unscale the coefficients back to their original scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unscale_coeffs(coeffs_reconstructed, scaling_factors, bits, do_plot=False):\n",
    "    coeffs_unscaled = {}\n",
    "\n",
    "    for key in coeffs_reconstructed.keys():\n",
    "        tmp_coeffs_unscaled = coeffs_reconstructed[key]/(2**bits)\n",
    "        tmp_coeffs_unscaled = tmp_coeffs_unscaled*scaling_factors[key]['scale_factor']\n",
    "        tmp_coeffs_unscaled = tmp_coeffs_unscaled + scaling_factors[key]['shift_factor']\n",
    "\n",
    "        #now replace the NaN values with 0\n",
    "        nan_inds = np.where(np.isnan(tmp_coeffs_unscaled))[0]\n",
    "        tmp_coeffs_unscaled[nan_inds] = 0\n",
    "\n",
    "        coeffs_unscaled[key] = tmp_coeffs_unscaled\n",
    "\n",
    "    return coeffs_unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate the lowest possible number of bits to quantize the wavelet coefficients such that the PRD is above the threshold\n",
    "1. quantize the signal starting at 8 bits\n",
    "1. unquantize and reconstruct the signal\n",
    "1. calculate the PRD. Repeat with 1 fewer bit (ie, 7 bits)\n",
    "1. repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_num_bits(orig_sig, coeffs_scaled, binary_map, scaling_factors):\n",
    "    #starting at 12 bits, keep decreasing the number of bits in the quantization\n",
    "    #until the PRD is above some threshold\n",
    "    num_bits = 13\n",
    "\n",
    "    #initialize PRD to 0 so the while loop can run\n",
    "    PRD = 0\n",
    "\n",
    "    #keep track of PRD per number of bits\n",
    "    PRD_dict = {}\n",
    "\n",
    "    while (num_bits >= 5) and (PRD <= MAX_PRD):\n",
    "        #decrement the number of bits\n",
    "        num_bits = num_bits-1\n",
    "\n",
    "        coeffs_quantized = do_quantization(coeffs_scaled, num_bits)\n",
    "\n",
    "        #rescale the coefficients\n",
    "        coeffs_unscaled = unscale_coeffs(None, coeffs_quantized, scaling_factors, num_bits)\n",
    "\n",
    "        #do the inverse dwt\n",
    "        data_reconstructed = wavelet_reconstruction(coeffs_unscaled, None, None)\n",
    "        \n",
    "        #calculate PRD\n",
    "        PRD = calculate_PRD(orig_sig, data_reconstructed)\n",
    "        PRD_dict[num_bits] = PRD\n",
    "\n",
    "    #if we went over the PRD, go back up by one bit\n",
    "    if PRD > MAX_PRD:\n",
    "        num_bits = num_bits+1\n",
    "        PRD = PRD_dict[num_bits]\n",
    "\n",
    "    return num_bits, PRD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine all the wavelet coefficients into one continuous array\n",
    "*done for each decomposition level and for binary map*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_coefficients(coeffs, binary_map=None):\n",
    "    coeffs_combined = []\n",
    "\n",
    "    #loop through each of the wavelet decompositions\n",
    "    #(or coefficient matrices) and remove all zero values\n",
    "    #based on the binary map\n",
    "    if binary_map is not None:\n",
    "        for key in coeffs.keys():\n",
    "            inds_to_keep = np.where(binary_map[key]==1)[0]\n",
    "            coeffs[key] = coeffs[key][inds_to_keep]\n",
    "\n",
    "    #add each array to coeffs_combined\n",
    "    coeffs_combined.extend(coeffs['cA5'])\n",
    "    coeffs_combined.extend(coeffs['cD5'])\n",
    "    coeffs_combined.extend(coeffs['cD4'])\n",
    "    coeffs_combined.extend(coeffs['cD3'])\n",
    "    coeffs_combined.extend(coeffs['cD2'])\n",
    "    coeffs_combined.extend(coeffs['cD1'])\n",
    "\n",
    "    return coeffs_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map the wavelet coefficients (and the binary map) back to their original decomposition levels\n",
    "*necessary prerequisite for reconstruction of the time domain waveform*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_coeffs(coeffs, binary_map):\n",
    "    coeffs_remapped = np.zeros(len(binary_map))*np.nan\n",
    "    inds_to_set = np.where(np.asarray(binary_map)==1)[0]\n",
    "    coeffs_remapped[inds_to_set] = coeffs\n",
    "\n",
    "    wavelet_remapped = {}\n",
    "    counter = 0\n",
    "    wavelet_remapped['cA5'] = coeffs_remapped[counter:counter+COEFF_LENGTHS['cA5']]\n",
    "\n",
    "    counter = counter + COEFF_LENGTHS['cA5']\n",
    "    wavelet_remapped['cD5'] = coeffs_remapped[counter:counter+COEFF_LENGTHS['cD5']]\n",
    "\n",
    "    counter = counter + COEFF_LENGTHS['cD5']\n",
    "    wavelet_remapped['cD4'] = coeffs_remapped[counter:counter+COEFF_LENGTHS['cD4']]\n",
    "\n",
    "    counter = counter + COEFF_LENGTHS['cD4']\n",
    "    wavelet_remapped['cD3'] = coeffs_remapped[counter:counter+COEFF_LENGTHS['cD3']]\n",
    "\n",
    "    counter = counter + COEFF_LENGTHS['cD3']\n",
    "    wavelet_remapped['cD2'] = coeffs_remapped[counter:counter+COEFF_LENGTHS['cD2']]\n",
    "\n",
    "    counter = counter + COEFF_LENGTHS['cD2']\n",
    "    wavelet_remapped['cD1'] = coeffs_remapped[counter:counter+COEFF_LENGTHS['cD1']]\n",
    "\n",
    "    return wavelet_remapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quantization\n",
    "*input: the selected largest wavelet coefficients*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_quantization(coeffs, bits):\n",
    "    quantized_coeffs = {}\n",
    "\n",
    "    for key in coeffs.keys():\n",
    "        sig = coeffs[key]\n",
    "        sig = sig*(2**bits-1)\n",
    "        sig = np.round(sig)\n",
    "        sig = np.array(sig).astype(int)\n",
    "\n",
    "        quantized_coeffs[key] = sig\n",
    "        \n",
    "    return quantized_coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compress the wavelet coefficients\n",
    "*combine bits into bytes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_coefficients(coeffs, num_bits):\n",
    "\n",
    "    binary_string = ''\n",
    "\n",
    "    for coeff in coeffs:\n",
    "        #convert each coefficient value to binary in num_bits number of bits\n",
    "        binary_string = binary_string + format(coeff, '0%ib' % num_bits)\n",
    "\n",
    "    #loop through sets of 8 bits in the binary string and convert to a byte\n",
    "    byte_array = []\n",
    "    for i in range(int(len(binary_string)/8)):\n",
    "        byte_tmp = binary_string[i*8:(i+1)*8]\n",
    "        byte_tmp = int(byte_tmp, 2)\n",
    "        byte_array.append(byte_tmp)\n",
    "\n",
    "    #check if there are any remaining bits that don't divide evenly into 8\n",
    "    #note the number of bits in this last byte for conversion back to int\n",
    "    #later on\n",
    "    num_bits_last_byte = 8\n",
    "    if len(binary_string)%8 != 0:\n",
    "        byte_tmp = binary_string[(i+1)*8:(i+1)*8 + len(binary_string)%8]\n",
    "        num_bits_last_byte = len(byte_tmp)\n",
    "        byte_tmp = int(byte_tmp, 2)\n",
    "        byte_array.append(byte_tmp)\n",
    "\n",
    "    return byte_array, num_bits_last_byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decompress the previously compressed wavelet coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_coefficients(coeffs_compressed, num_bits, num_bits_last_byte):\n",
    "\n",
    "    binary_string = ''\n",
    "\n",
    "    #convert each coefficient value to binary in 8 number of bits\n",
    "    #note that the very last value in the the binary map may not be\n",
    "    #a full 8 bits. so convert that based on num_bits_last_byte\n",
    "    coeffs_len = len(coeffs_compressed)\n",
    "    for i in range(coeffs_len):\n",
    "        if i == coeffs_len-1:\n",
    "            binary_string = binary_string + format(coeffs_compressed[i], '0%ib' % num_bits_last_byte)\n",
    "        else:\n",
    "            binary_string = binary_string + format(coeffs_compressed[i], '08b')\n",
    "\n",
    "\n",
    "    #loop through sets of num_bits bits in the binary string and convert to a byte\n",
    "    byte_array = []\n",
    "    for i in range(int(len(binary_string)/num_bits)):\n",
    "        byte_tmp = binary_string[i*num_bits:(i+1)*num_bits]\n",
    "        byte_tmp = int(byte_tmp, 2)\n",
    "        byte_array.append(byte_tmp)\n",
    "\n",
    "\n",
    "    return byte_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compress the binary map using variable length run-length encoding (RLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_binary_map(binary_map):\n",
    "    #define a state machine that loops through each entry in the binary map and \n",
    "    #creates the compressed representation. \n",
    "\n",
    "    #the last run count won't be included in the compressed representation, so \n",
    "    #just append one more value at the end of the binary map to trigger the last \n",
    "    #compression value. make a local deep copy so that the original is not affected\n",
    "    binary_map = copy.deepcopy(binary_map)\n",
    "    binary_map.append(int(not binary_map[-1]))\n",
    "\n",
    "\n",
    "    CURRENT_STATE = binary_map[0]\n",
    "    run_count = 0\n",
    "    binary_string = ''\n",
    "\n",
    "    #loop through each value in the binary map\n",
    "    for val in binary_map:\n",
    "\n",
    "        #if the current binary map value is the same as the previous one, just increment the run count\n",
    "        if val == CURRENT_STATE:\n",
    "            run_count = run_count + 1\n",
    "\n",
    "        #otherwise, encode the current run count \n",
    "        else:\n",
    "\n",
    "            #handle cases where run count <= 3\n",
    "            if run_count == 1:\n",
    "                binary_string_tmp = '00'\n",
    "\n",
    "            elif run_count == 2:\n",
    "                binary_string_tmp = '01'\n",
    "\n",
    "            elif run_count == 3:\n",
    "                binary_string_tmp = '10'\n",
    "\n",
    "            #otherwise, if the run count > 3\n",
    "            else:\n",
    "                #calculate the number bits required to represent the run count\n",
    "                num_bits_run_count = len(format(run_count, 'b'))\n",
    "\n",
    "                #build a binary string\n",
    "                binary_string_tmp = ''\n",
    "\n",
    "                #first bit represents that the run count > 3\n",
    "                binary_string_tmp = binary_string_tmp + '11'\n",
    "\n",
    "                #next 4 bits represent the number of bits that will define the run count\n",
    "                binary_string_tmp = binary_string_tmp + format(num_bits_run_count, '0%ib' % NUM_BITS_RUN_LEN)\n",
    "\n",
    "                #next number of bits is variable, and is the actual run count\n",
    "                #may be up to 15 bits assuming NUM_BITS_RUN_LEN=4\n",
    "                binary_string_tmp = binary_string_tmp + format(run_count, 'b')\n",
    "\n",
    "            #print(str(run_count) + ', ' + binary_string_tmp)\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            #append the binary string\n",
    "            binary_string = binary_string + binary_string_tmp\n",
    "\n",
    "            #reset the run count \n",
    "            run_count = 1\n",
    "\n",
    "        #update the current state\n",
    "        CURRENT_STATE = val\n",
    "\n",
    "\n",
    "    #convert the binary string into a buffer of 8 bit bytes \n",
    "    byte_array = []\n",
    "    for i in range(int(len(binary_string)/8)):\n",
    "        byte_tmp = binary_string[i*8:(i+1)*8]\n",
    "        byte_tmp = int(byte_tmp, 2)\n",
    "        byte_array.append(byte_tmp)\n",
    "\n",
    "\n",
    "    #check if there are any remaining bits that don't divide evenly into 8\n",
    "    num_bits_last_byte = 8\n",
    "    if len(binary_string)%8 != 0:\n",
    "        byte_tmp = binary_string[(i+1)*8:(i+1)*8 + len(binary_string)%8]\n",
    "        num_bits_last_byte = len(byte_tmp)\n",
    "        byte_tmp = int(byte_tmp, 2)\n",
    "        byte_array.append(byte_tmp)\n",
    "\n",
    "\n",
    "    #return the initial state (ie, the first value in binary map), and the RLE binary map\n",
    "    return binary_map[0], byte_array, num_bits_last_byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decompress the previously compressed binary map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_binary_map(binary_map_compressed, binary_map_initial_state, num_bits_last_byte):\n",
    "\n",
    "    #first convert 8 bit numbers into a binary string\n",
    "    binary_string = ''\n",
    "\n",
    "    #convert each coefficient value to binary in 8 number of bits\n",
    "    #note that the very last value in the the binary map may not be\n",
    "    #a full 8 bits. so convert that based on num_bits_last_byte\n",
    "    binary_map_len = len(binary_map_compressed)\n",
    "    for i in range(binary_map_len):\n",
    "        if i == binary_map_len-1:\n",
    "            binary_string = binary_string + format(binary_map_compressed[i], '0%ib' % num_bits_last_byte)\n",
    "        else:\n",
    "            binary_string = binary_string + format(binary_map_compressed[i], '08b')\n",
    "\n",
    "\n",
    "    #define a state machine that loops through each entry in the binary map and \n",
    "    #creates the uncompressed representation. \n",
    "    READ_HEADER = 0\n",
    "    READ_NUM_BITS = 1\n",
    "    READ_RUN_LEN = 2\n",
    "    state = READ_HEADER\n",
    "\n",
    "    run_type = binary_map_initial_state\n",
    "    header = ''\n",
    "    binary_array = np.array([])\n",
    "\n",
    "\n",
    "    #loop through each value in the binary map\n",
    "    for val in binary_string:\n",
    "\n",
    "        #read the header\n",
    "        if state == READ_HEADER:\n",
    "            header = header + val\n",
    "\n",
    "            if len(header) == 2:\n",
    "                #run count 1\n",
    "                if header == '00':\n",
    "                    binary_array = np.concatenate((binary_array, np.ones(1)*run_type))\n",
    "                    run_type = int(not run_type)\n",
    "                    state = READ_HEADER\n",
    "\n",
    "                #run count 2\n",
    "                if header == '01':\n",
    "                    binary_array = np.concatenate((binary_array, np.ones(2)*run_type))\n",
    "                    run_type = int(not run_type)\n",
    "                    state = READ_HEADER\n",
    "\n",
    "                #run count 3\n",
    "                if header == '10':\n",
    "                    binary_array = np.concatenate((binary_array, np.ones(3)*run_type))\n",
    "                    run_type = int(not run_type)\n",
    "                    state = READ_HEADER\n",
    "\n",
    "                #run count > 3\n",
    "                if header == '11':\n",
    "                    state = READ_NUM_BITS\n",
    "                    num_bits = ''\n",
    "\n",
    "\n",
    "                #reset header \n",
    "                header = ''\n",
    "\n",
    "            continue\n",
    "\n",
    "        #read number of bits\n",
    "        if state == READ_NUM_BITS:\n",
    "\n",
    "\n",
    "            num_bits = num_bits + val\n",
    "\n",
    "            if len(num_bits) == 4:\n",
    "                num_bits_run_len = int(num_bits, 2)\n",
    "                run_len = ''\n",
    "\n",
    "                state = READ_RUN_LEN\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        #read run length\n",
    "        if state == READ_RUN_LEN:\n",
    "            run_len = run_len + val\n",
    "\n",
    "            if len(run_len) == num_bits_run_len:\n",
    "                run_len = int(run_len, 2)\n",
    "                binary_array = np.concatenate((binary_array, np.ones(run_len)*run_type))\n",
    "                run_type = int(not run_type)\n",
    "                state = READ_HEADER\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "    return binary_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate signal energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(sig):\n",
    "    return np.sum(sig**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
